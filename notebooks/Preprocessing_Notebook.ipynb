{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy, pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `stft` and `istft` functions from [Steve Tjoa](http://stackoverflow.com/questions/2459295/invertible-stft-and-istft-in-python). Modified it to use rfft so we only have the positive part, and get the DFT by taking the absolute value, a la [jojek](http://dsp.stackexchange.com/questions/20500/negative-values-of-the-fft). Additionally, we want to zero-pad to improve our resolution (although that doesn't do as much as one would hope). For more on that, read [hotpaw2's answer](http://dsp.stackexchange.com/questions/741/why-should-i-zero-pad-a-signal-before-taking-the-fourier-transform) on StackOverflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stft(x, fs, framesz, hop, pad=0):\n",
    "    framesamp = int(framesz*fs)\n",
    "    hopsamp = int(hop*fs)\n",
    "    w = scipy.hanning(framesamp)\n",
    "    X = np.abs(scipy.array([np.fft.rfft(np.lib.pad(w*x[i:i+framesamp], (framesamp*pad,), 'constant', constant_values=(0,)))\n",
    "                     for i in range(0, len(x)-framesamp, hopsamp)]))/framesamp*2\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm creating a `TrainingData` type to store the audio and onset/offset data, as well as handle the preprocessing of the audio and the memory management. I'll likely have to create some MAPSReader as well to construct paths and load the MAPS data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainingData:\n",
    "    def __init__(self, file_name):\n",
    "        # read in the audio of the file\n",
    "        self.rate, self.x = scipy.io.wavfile.read(file_name+'.wav')\n",
    "        self.duration = len(self.x) / self.rate\n",
    "        # read in the training data\n",
    "        self.notes = pd.read_csv(file_name+'.txt', sep='\\t')\n",
    "        self.notes.columns = ['onset_time', 'offset_time', 'midi_pitch']\n",
    "        # some key constants\n",
    "        self.window_size = 0.050\n",
    "        self.hop_size = 0.025\n",
    "        self.pad = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the function to preprocess the MAPS data. It windows and performs the stft of the audio data, convolves the data with a filter bank, and then lines up each window with the notes occuring at that time.\n",
    "\n",
    "The shape that stft returns is [windows, fourier values]. In the below code, framesz is the width of the frame in seconds (in our case, we are using 50ms) and hop is the stride of the frames (because we want 50% overlap, we're using a hop of 25ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform(self):\n",
    "    self.X = stft(self.x[:, 0], self.rate, self.window_size, hop=self.hop_size, pad=self.pad)\n",
    "    self.F = np.fft.rfftfreq(int(self.rate*self.window_size*(2*self.pad+1)), 1.0/self.rate)\n",
    "    \n",
    "    # clean up the variables we will no longer be using\n",
    "    del self.x\n",
    "\n",
    "TrainingData.transform = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(self, filterbank):\n",
    "    count = np.shape(self.X)[0]\n",
    "    self.data = np.zeros((count, filterbank.width))\n",
    "    for i in range(count):\n",
    "        self.data[i, :] = filterbank.apply_filterbank(self.X[i, :])\n",
    "    \n",
    "TrainingData.preprocess = preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "audio = TrainingData('../data/MUS/MAPS_MUS-alb_se7_AkPnBsdf')\n",
    "audio.transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Our Filters\n",
    "\n",
    "The equation we'll use for each semitone note is $ f_n = f_0 * a^n $ where $ a = 2^{1/12} $ and $ n $ is the number of semitones away from the fundamental frequency we are. We'll use $ f_0 $ = 440Hz for the central A. A lot of the preprocessing I'm doing (as in, almost all of it) is coming from [this paper](http://www.music-ir.org/evaluation/mirex-results/articles/onset/pertusa.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_freq(f0, count):\n",
    "    return f0 * math.pow(math.pow(2, 1.0/12.0), count)\n",
    "\n",
    "class FilterBank:\n",
    "    def __init__(self, fftfreqs):\n",
    "        self.bands = []\n",
    "        self.freqs = fftfreqs\n",
    "        self.diff = self.freqs[1]-self.freqs[0]\n",
    "        self.offset = self.freqs[0]/self.diff\n",
    "    \n",
    "    def discretize(self, value):\n",
    "        return int(round(value/self.diff)-self.offset)\n",
    "\n",
    "    def band(self, low, center, high):\n",
    "        l, c, h = self.discretize(low), self.discretize(center), self.discretize(high)\n",
    "        self.bands.append(Band(l, c, h))\n",
    "    \n",
    "    def construct_bands(self, f0, n_below, n_above):\n",
    "        for i in range(1, n_below+1)[::-1]:\n",
    "            self.band(compute_freq(f0, -i-1), compute_freq(f0, -i), compute_freq(f0, -i+1))\n",
    "        for i in range(n_above+1):\n",
    "            self.band(compute_freq(f0, i-1), compute_freq(f0, i), compute_freq(f0, i+1))\n",
    "        self.width = n_below + n_above + 1\n",
    "    \n",
    "    def apply_filterbank(self, freqs):\n",
    "        values = np.zeros(len(self.bands))\n",
    "        for i, band in enumerate(self.bands):\n",
    "            values[i] = band.values.dot(freqs[band.low:band.high+1]) / band.total\n",
    "        return values\n",
    "\n",
    "class Band:\n",
    "    def __init__(self, low, center, high):\n",
    "        self.low, self.center, self.high = low, center, high\n",
    "        self.construct()\n",
    "        \n",
    "    def construct(self):\n",
    "        f, s = self.center-self.low, self.high-self.center\n",
    "        self.values = np.zeros(self.high-self.low+1)\n",
    "        for i in range(f):\n",
    "            self.values[i] = float(i)/f\n",
    "        for i in range(s+1):\n",
    "            self.values[f+i] = float(s-i)/s\n",
    "        self.total = np.sum(self.values)\n",
    "\n",
    "\n",
    "f = FilterBank(audio.F.flatten())\n",
    "f.construct_bands(440.0, 20, 20)\n",
    "\n",
    "np.shape(f.apply_filterbank(audio.X[50, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now preprocess the data using our `FilterBank`. The current `FilterBank` has 41 notes centered around A at 440Hz. When we call `preprocess` on our TrainingData instance, it applies the filterbank over the length of the musical piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "audio.preprocess(f)\n",
    "print np.shape(audio.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can see what the output looks like. Pretty promising, no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(audio.data[0:100, :].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Label Data\n",
    "\n",
    "In addition to the actual audio data, we need the align the note labels to the music, in a vector format. Since we're using [midi pitches](https://www.finalemusic.com/UserManuals/Finale2012Mac/Content/Finale/MIDI_Note_to_Pitch_Table.htm), we'll have a length 88 vector with several-hot notation.\n",
    "\n",
    "For instnace, if our vector was length 5, and we had notes 2 and 3 on at a specific time, our resulting vector would be: \n",
    "```[0, 0, 1, 1, 0]``` (assuming 0-indexed). Note that MIDI pitches are 21-indexed, so we have to account for the offset. The pitches in the training data thus range from 21 to 108."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label(self):\n",
    "    count = np.shape(self.X)[0]\n",
    "    self.Y = np.zeros((count, 88))\n",
    "    factor = (self.window_size - self.hop_size)\n",
    "    for i, row in self.notes.iterrows():\n",
    "        onset_index = int(math.floor(row.onset_time / factor + self.hop_size))\n",
    "        offset_index = int(round(row.offset_time / factor + self.hop_size))\n",
    "        self.Y[onset_index:offset_index, int(row.midi_pitch-21)] = 1.0\n",
    "\n",
    "TrainingData.label = label\n",
    "\n",
    "audio.label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(audio.Y[0:100, :].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping the Filterbank\n",
    "\n",
    "Using information from [this post](http://electronics.stackexchange.com/questions/12407/what-is-the-relation-between-fft-length-and-frequency-resolution) I compute the resolution of the fourier transform and create a filterbank from that, rather than the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = (44100 * 0.05 * (2 * 5 + 1)) / 2\n",
    "resolution = 22050 / bins\n",
    "frequencies = np.array([resolution*i for i in range(int(round(bins)))])\n",
    "\n",
    "for i, b in enumerate(audio.F):\n",
    "    if b != frequencies[i]:\n",
    "        print \"Found one at:\", i\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
